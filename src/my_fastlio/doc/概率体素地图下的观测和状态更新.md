# 概率体素地图下的观测和状态更新

观测方程 $0 = \mathbf{h}_i(\mathbf{x}_k, \delta{\mathbf{p}}, \delta{\mathbf{q}}, \delta{\mathbf{u}})$ : 
$$
\begin{equation}
  \begin{aligned}
    0 &= {}^{G}\mathbf{u}_{I_k}^{T} ({}^{G}\mathbf{R}_{I_k} ( {}^{I}\mathbf{R}_L {}^{L}\mathbf{p}_j + {}^{I}\mathbf{p}_L ) + {}^{G}\mathbf{p}_{I_k} - {}^{G}\mathbf{q}_{j} )\\

    &= ( {}^{G}\hat{\mathbf{u}}_{I_k} \boxplus \delta{\mathbf{u}} )^{T} \left( {}^{G}\mathbf{\hat{R}}_{I_i} \boxplus {}^{G}\mathbf{\widetilde{\theta}}_{I_i} \left( {}^{I}\mathbf{\hat{R}}_{L} \boxplus {}^{I}\mathbf{\widetilde{\theta}}_{L} ( {}^{L}\mathbf{\hat{p}}_{j} + \delta{\mathbf{p}} ) + {}^{L}\mathbf{\hat{p}}_{L} \boxplus {}^{L}\mathbf{\widetilde{p}}_{L} \right) + {}^{G}\mathbf{\hat{p}}_{I_k} \boxplus {}^{G}\mathbf{\widetilde{p}}_{I_k} - {}^{G}\mathbf{\hat{q}}_{j} - \delta{\mathbf{q}} \right)
  \end{aligned}
\end{equation}
$$

其中法向量 $ \delta{\mathbf{u}} \sim N(0, \varSigma_u ) $ , 点 $\delta{\mathbf{p}}, \delta{\mathbf{q}} \sim N(0, \varSigma_p)$, 上式经过推导还是可以得出:

$$
\begin{equation}
  \begin{aligned}
    0 &\simeq \underbrace{{}^{G}\mathbf{\hat{u}}_{I_k}^{T} ({}^{G}\mathbf{\hat{R}}_{I_k} ( {}^{I}\mathbf{\hat{R}}_L  {}^{L}\mathbf{\hat{p}}_j + {}^{I}\mathbf{\hat{p}}_L ) + {}^{G}\mathbf{\hat{p}}_{I_k} - {}^{G}\mathbf{\hat{q}}_{j} )}_{\mathbf{h}_k(\mathbf{\hat{x}}_k, 0, 0, 0)} + \mathbf{H}_x \mathbf{\widetilde{x}}_x + \mathbf{H}_u \delta{\mathbf{u}} + \mathbf{H}_q \delta{\mathbf{q}} + \mathbf{H}_p \delta{\mathbf{p}} \\
    &= \mathbf{z}_k + \mathbf{H}_x \mathbf{\widetilde{x}}_k + \mathbf{v}_j 
  \end{aligned}
\end{equation}
$$

$$
\begin{equation}
  \begin{aligned}
    \mathbf{z}_k + \mathbf{H}_x \mathbf{\widetilde{x}}_k = -\mathbf{v}_j
  \end{aligned}
\end{equation}
$$

这里的 $\mathbf{z}_k$ 就是观测值, $\mathbf{H}_x$ 就是观测矩阵, 上一节[观测模型推导](./观测模型推导.md)其实推导过,但是当时忘记在 $\mathbf{T}$ 中分离 $\mathbf{R}, \mathbf{p}$ ,导致形式和论文中不一致,(我是个大傻子,哭死了), 不想在写latex了用论文中的形式吧, $\mathbf{H}_u, \mathbf{H}_p, \mathbf{H}_q$ 就是服从正态分布的 $\delta{\mathbf{u}} , \delta{\mathbf{p}}, \delta{\mathbf{q}}$ 的转移矩阵. 这几个矩阵会在最后给出具体形式,暂时先不讨论. 这三者由于服从均值为0的正态分布, 线型变化后的 $\mathbf{v}_j$ 仍然服从均值为0的正态分布, 那么就可以用 $\mathbf{z}_k + \mathbf{H}_x \mathbf{\widetilde{x}}_k$ 构造最大后验估计:

$$
\begin{equation}
  \begin{aligned}
    \min_{\mathbf{\widetilde{x}_k}} \left( \| \mathbf{\widetilde{x}}_k \|_{\mathbf{P}_k}^{2} + \sum \| \mathbf{z}_k + \mathbf{H}_x \mathbf{\widetilde{x}}_k \|_{\mathbf{R}_j}^{2} \right)
  \end{aligned}
\end{equation}
$$

当然则是ESKF的步骤, 对于 IESKF, 其误差状态在迭代更新的时候不断在改变 $\mathbf{\widetilde{x}}_k^{n-1} \rightarrow \mathbf{\widetilde{x}}_k^{n} $ , 即此时观测的线型化点在不断改变(即上式中的 $\mathbf{H}_x$ 后的 $\mathbf{\widetilde{x}_k}$ 和 $\mathbf{x}_k \boxminus \mathbf{\hat{x}}_k $ 在不断改变), 此时的 $\mathbf{P}_k $ 已经不能表示它的状态分布了, 因此我们需要计算这种改变,重新推导 $\mathbf{P}_k$ .
$$
\begin{equation}
  \begin{aligned}
    
    \mathbf{\widetilde{x}}_k = \mathbf{x}_k \boxminus \mathbf{\hat{x}}_k = (\mathbf{\hat{x}}_k^{n} \boxplus \mathbf{\widetilde{x}}_k^{n}) \boxminus \mathbf{\hat{x}}_k^{0} \triangleq \mathbf{f}(\mathbf{x})|_{\mathbf{x} = \mathbf{\widetilde{x}}_{k}^{n}}
  \end{aligned}
\end{equation}
$$ 

在 $ \mathbf{x} = 0$ 处泰勒展开:

$$
\begin{equation}
  \begin{aligned}
    \mathbf{f}(\mathbf{\widetilde{x}}_k^n) = \mathbf{\hat{x}}_k^n \boxminus \mathbf{\hat{x}}_k^{0} + \mathbf{J}^n (\mathbf{\widetilde{x}}_k^n - 0)
  \end{aligned}
\end{equation}
$$

和论文内容一致,用一个图表示就是:
<p align="center">
  <img src="update_illustrate.png" alt="描述信息" width="300">
</p>

计算 $\mathbf{J}^n$:
$$
\begin{equation}
  \begin{aligned}
    \mathbf{J}^n = \left. \frac{\mathbf{f}(\mathbf{x})}{\partial \mathbf{x}} \right|_{\mathbf{x} = \mathbf{0}}
  \end{aligned}
\end{equation}
$$

这和下面形式等价:
$$
\begin{equation}
  \begin{aligned}
    \frac{\partial \left(\mathbf{a} \boxplus \mathbf{b} \right) \boxminus \mathbf{c}}{\partial \mathbf{b}}
  \end{aligned}
\end{equation}
$$

$$
\begin{equation}
  \begin{aligned}
    \mathbf{A} &= \left(\mathbf{a} \boxplus \mathbf{b} \right) \boxminus \mathbf{c} = \mathbf{Log}(\mathbf{c}^T \mathbf{a} \mathbf{Exp}(\mathbf{b}))\\
    \mathbf{Exp}(\mathbf{A}) &= \mathbf{c}^T \mathbf{a} \mathbf{Exp}(\mathbf{b}) \\
    \mathbf{Exp}(\mathbf{A} + \mathbf{\Delta{A}}) &= \mathbf{c}^T \mathbf{a} \mathbf{Exp}(\mathbf{b} + \delta{\mathbf{b}}) \\
    \mathbf{Exp}(\mathbf{A}) \mathbf{Exp}(\mathbf{J}_{rl} \mathbf{\Delta{A}}) &= \mathbf{c}^T \mathbf{a} \mathbf{Exp}(\mathbf{b}) \mathbf{Exp}(\mathbf{J}_{rr} \delta{\mathbf{b}}) \\ 
    \overset{两边消除}{\rightarrow} \mathbf{Exp}(\mathbf{J}_{rl} \mathbf{\Delta{A}}) &= \mathbf{Exp}(\mathbf{J}_{rr} \delta{\mathbf{b}}) \\
    \mathbf{J}_{rl} \mathbf{\Delta{A}} &= \mathbf{J}_{rr} \delta{\mathbf{b}}\\
    \frac{\Delta{\mathbf{A}}}{\delta{\mathbf{b}}} &= \frac{\mathbf{J}_{rr}}{\mathbf{J}_{rl}}
  \end{aligned}
\end{equation}
$$

即:

$$
\begin{equation}
  \begin{aligned}
    \mathbf{J}^n = \left. \frac{\mathbf{J}_r(\mathbf{x})} {\mathbf{J}_r((\mathbf{\hat{x}}_k^{n} \boxplus \mathbf{x}) \boxminus \mathbf{\hat{x}}_k^{0})} \right|_{\mathbf{x} = \mathbf{0}} = \mathbf{J}_r(\mathbf{\hat{x}}_k^{n} \boxminus \mathbf{\hat{x}}_k^0)^{-1}
  \end{aligned}
\end{equation}
$$:

$\mathbf{x}$在上一节 [前向传播](./前向传播.md) 中定义:
$$
\begin{equation}
  \begin{aligned}
    \mathbf{J}^n = \mathrm{x}\triangleq
    \begin{bmatrix}
     ^{G}\mathrm{p}_{I}^{T} & ^{G}\mathrm{R}_{I}^{T} & ^{I}\mathbf{R}_{L}^{T} & ^{I}\mathbf{p}_{L}^{T} & ^{G}\mathrm{v}_{I}^{T} & \mathrm{b}_{\boldsymbol{\omega}}^{T} & \mathbf{b}_{\mathbf{a}}^{T} & ^{G}\mathbf{g}^{T}
    \end{bmatrix}^{T}\in\mathcal{M}
  \end{aligned}
\end{equation}
$$

很容易得到 $\mathbf{J}^n$:
$$
\begin{equation}
  \begin{aligned}
  \begin{bmatrix}
  \mathbf{I}_{3\times3} & \mathbf{0} & \mathbf{0} & \mathbf{0} \\

  \mathbf{0} & \mathbf{J}_r(^{G}\mathrm{\hat{R}}_{I_k}^n \boxminus {}^{G}\mathrm{\hat{R}}_{I_k}^0)^{-1} & \mathbf{0} & \mathbf{0} \\

  \mathbf{0} & \mathbf{0} & \mathbf{J}_r(^{I}\mathrm{\hat{R}}_{L_k}^n \boxminus {}^{I}\mathrm{\hat{R}}_{L_k}^0)^{-1} & \mathbf{0} \\

  \mathbf{0} & \mathbf{0} & \mathbf{0} & \mathbf{I}_{14\times14}

  \end{bmatrix}_{23\times23}
  \end{aligned}
\end{equation}
$$

和论文中是一致的.

现在就可以改写最大后验估计了:

$$
\begin{equation}
  \begin{aligned}
    \min_{\mathbf{\widetilde{x}_k^n}} \left( \| \mathbf{\widetilde{x}}_k^n \|_{\mathbf{P}_k^n}^{2} + \sum \| \mathbf{z}_k^n + \mathbf{H}_x^{n} \mathbf{\widetilde{x}}_k^n \|_{\mathbf{R}_j}^{2} \right)
  \end{aligned}
\end{equation}
$$

这里 $\|\mathbf{x}\|_{\mathbf{M}}^2 = \mathbf{x}^T \mathbf{M}^{-1} \mathbf{x}$ ,这里带上标 $n$ 的都是在迭代过程中需要更新的量, 比较麻烦的就是这个更新, 利用最大后验证估计进行更新, 以后在推导吧,现在先引用王泽霖大佬的文章吧:
[FAST-LIO推导](https://zhuanlan.zhihu.com/p/587500859)

对于 $\mathbf{H}_u, \mathbf{H}_q, \mathbf{H}_p$ 的推导和之前一样,都是另其他分量为 0,很容易求得:
$$
\begin{equation}
  \begin{aligned}
    \mathbf{H}_u = \left( {}^{G}\mathbf{\hat{R}}_{I_k} ( {}^{I}\mathbf{\hat{R}}_L {}^{L}\mathbf{\hat{p}}_j + {}^{I}\mathbf{\hat{p}}_L ) + {}^{G}\mathbf{\hat{p}}_{I_k} - {}^{G}\mathbf{\hat{q}}_{j} \right)^T
  \end{aligned}
\end{equation}
$$

$$
\begin{equation}
  \begin{aligned}
    \mathbf{H}_q = {}^{G}\mathbf{u}_{I_k}^T
  \end{aligned}
\end{equation}
$$

$$
\begin{equation}
  \begin{aligned}
    \mathbf{H}_p = {}^{G}\mathbf{u}_{I_k}^T {}^{G}\mathbf{\hat{R}}_{I_k} {}^{I}\mathbf{\hat{R}}_L
  \end{aligned}
\end{equation}
$$

即当 $\mathbf{v}_j \sim N(0, \varSigma_v) $ 时 
